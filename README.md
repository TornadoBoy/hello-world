# hello-world

1. Rigorous evaluation and comparison requires testing and validation of tools against recent and complete traffic traces, which will require navigating the persistent challenges (mostly policy, some technical) of sharing traffic data with researchers.

2. The ever increasing speed of network links requires rigorous investigation of scalability trade-offs in traffic classification. Appropriate and novel designs for highly parallel low-cost architectures promise significant scalability improvements.

3. Improving tools to annotate data with the actual traffic class (i.e., ground truth tools) can be done through sharing of algorithms and signatures in order to allow community contributions, comparisons, and validation, for example, by comparing the output of the annotating tools against 100 percent safe reference data.

4. Traffic classification techniques and algorithms should be presented with rigorous empirically grounded analysis of efficiency and performance, using standard metrics comparing implementations running on diverse Internet traffic, including encapsulated, encrypted, and multichannel application flows of varying length.

5. Research on multiclassifier systems is warranted, since they combine the benefits of different approaches to improve accuracy, flexibility, and speed, at some cost in computational complexity and possibly additional training data and time.

6. Publications of open source implementations of real traffic classification systems for use in experiments would foster collaboration and promote convergence on standard definitions, procedures, and reliable evaluation of techniques.
